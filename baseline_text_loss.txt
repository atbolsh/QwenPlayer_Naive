Baseline Text Cross-Entropy Loss (Untrained Qwen3-0.6B)
==================================================
Model: Qwen/Qwen3-0.6B
Dataset: control text samples (sdt/ProcessBench)
Batch size: 32
Num batches: 10
Total samples: 320

Results:
  Mean loss:   3.0016
  Std dev:     0.1022
  Min loss:    2.8750
  Max loss:    3.2031
  Perplexity:  20.12

Individual batch losses:
  Batch 1: 3.0469
  Batch 2: 3.2031
  Batch 3: 2.9688
  Batch 4: 2.9375
  Batch 5: 3.0781
  Batch 6: 3.0938
  Batch 7: 2.8750
  Batch 8: 2.9688
  Batch 9: 2.9531
  Batch 10: 2.8906

Interpretation:
  - If trained QwenAgent text loss >> this baseline, something is broken
  - If trained QwenAgent text loss ~= this baseline, model preserved capabilities
  - If trained QwenAgent text loss << this baseline, model improved on this task
